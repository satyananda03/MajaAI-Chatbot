{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "EMBEDDING_PROVIDER=\"aws\"\n",
    "EMBEDDING_MODEL_ID=\"amazon.titan-embed-text-v2:0\"\n",
    "LLM_PROVIDER=\"aws\"\n",
    "LLM_ID = \"apac.amazon.nova-lite-v1:0\"\n",
    "MAX_TOKEN = 1500\n",
    "TEMPERATURE = 0.0\n",
    "STREAMING = True\n",
    "\n",
    "# DATABASE\n",
    "REDIS_URL=\"redis://localhost:6379\"\n",
    "PERSIST_DIR=\"./data/chroma_dbs\"\n",
    "COLLECTION_NAME=\"parent-child-chunk-v1\"\n",
    "BM25_INDEX_PATH = \"./data/bm25-index/bm25-index-v1.pkl\"\n",
    "STOPWORDS_PATH = \"./data/stopwords.txt\"\n",
    "\n",
    "# RETRIEVER\n",
    "RETRIEVAL_STRATEGY: str = \"hybrid\" \n",
    "MAX_RESULTS: int = 2\n",
    "BM25_SEARCH_K: int = 5\n",
    "VECTOR_SEARCH_K: int = 80\n",
    "BM25_WEIGHT: float = 0.3\n",
    "VECTOR_WEIGHT: float = 0.7 \n",
    "RRF_CONSTANT: int = 5\n",
    "SCORE_THRESHOLD: float = 0.01\n",
    "\n",
    "# PROMPT\n",
    "PROMPT_VERSION = \"v1\"\n",
    "PROMPT_DIR = \"./prompts\"\n",
    "\n",
    "# SPLITTER\n",
    "PARENT_CHUNK_SIZE=2400\n",
    "PARENT_CHUNK_OVERLAP=260\n",
    "CHILD_CHUNK_SIZE=300\n",
    "CHILD_CHUNK_OVERLAP=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class PDFFileLoader:\n",
    "    def load(self, source: str) -> List[Document]:\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(file_path=source)\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata[\"source_type\"] = \"pdf\"\n",
    "                doc.metadata[\"file_path\"] = source\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Gagal memuat PDF {source}: {str(e)}\"\n",
    "            raise RuntimeError(error_msg) from e\n",
    "\n",
    "class PDFDirectoryLoader:\n",
    "    def __init__(self):\n",
    "        self.single_loader = PDFFileLoader()\n",
    "\n",
    "    def load(self, source: str, recursive: bool = True) -> List[Document]:\n",
    "        documents: List[Document] = []\n",
    "        search_pattern = os.path.join(source, \"*.pdf\")\n",
    "        # Ambil list semua file path\n",
    "        pdf_files = glob.glob(search_pattern)\n",
    "        if not pdf_files:\n",
    "            return []\n",
    "        # Loop setiap file di dalam direktori\n",
    "        for file_path in pdf_files:\n",
    "            # Skip hidden files\n",
    "            if \"/.\" in file_path or \"\\\\.\" in file_path:\n",
    "                continue\n",
    "            # Panggil loader satuan\n",
    "            docs = self.single_loader.load(file_path)\n",
    "            documents.extend(docs)\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class ParentChildSplitter:\n",
    "    def __init__(self, parent_chunk_size: int, parent_chunk_overlap: int, child_chunk_size: int, child_chunk_overlap: int):\n",
    "        self.parent_splitter = RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size,\n",
    "                                                            chunk_overlap=parent_chunk_overlap,\n",
    "                                                            separators=[\"\\n\", \". \", \", \", \" \", \"\"])\n",
    "        self.child_splitter = RecursiveCharacterTextSplitter(chunk_size=child_chunk_size,\n",
    "                                                            chunk_overlap=child_chunk_overlap,\n",
    "                                                            separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"])\n",
    "                                                            \n",
    "    def _preprocess_text(self, chunk_text: str) -> Tuple[str, List[str]]:\n",
    "        # Ekstrak URL\n",
    "        urls = re.findall(r'https?://\\S+', chunk_text)\n",
    "        # Replace URL dengan placeholder LINK\n",
    "        text_clean = re.sub(r'https?://\\S+', '[LINK]', chunk_text)\n",
    "        # Hapus karakter yang tidak diinginkan tapi\n",
    "        text_clean = re.sub(r'[^\\w\\s\\[\\],.:()%=+\\-/]', '', text_clean)\n",
    "        # Rapikan newline ganda > 2 menjadi 1\n",
    "        text_clean = re.sub(r'\\n{2,}', '\\n', text_clean)\n",
    "        # Rapikan spasi berlebihan\n",
    "        text_clean = re.sub(r'[ ]{2,}', ' ', text_clean)\n",
    "        # Fix multiple blank lines -> single newline\n",
    "        text_clean = re.sub(r'\\n\\s*\\n+', '\\n', text_clean)\n",
    "        return text_clean.strip(), urls\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> Dict[str, List[Document]]:\n",
    "        # Grouping by Source/File Path\n",
    "        docs_by_source = defaultdict(list)\n",
    "        for doc in documents:\n",
    "            source_key = doc.metadata.get(\"file_path\")\n",
    "            docs_by_source[source_key].append(doc)\n",
    "        all_parent_docs = []\n",
    "        all_child_docs = []\n",
    "        # Proses per File Source\n",
    "        for source_name, doc_group in docs_by_source.items():\n",
    "            # Gabungkan semua text dari satu file (merge pages)\n",
    "            combined_text = \"\\n\".join([d.page_content for d in doc_group])\n",
    "            # Ambil metadata dasar dari halaman pertama\n",
    "            base_metadata = doc_group[0].metadata.copy() if doc_group else {}\n",
    "            # Buat satu dokumen besar sementara\n",
    "            combined_doc = Document(page_content=combined_text, metadata=base_metadata)\n",
    "            # Generate Parent Chunks\n",
    "            parent_chunks = self.parent_splitter.split_documents([combined_doc])\n",
    "            for p_doc in parent_chunks:\n",
    "                # Cleaning Text\n",
    "                clean_text, specific_urls = self._preprocess_text(p_doc.page_content)\n",
    "                # Generate Parent ID (UUID)\n",
    "                parent_id = str(uuid.uuid4())\n",
    "                # Update Metadata Parent\n",
    "                parent_meta = p_doc.metadata.copy()\n",
    "                parent_meta.update({\n",
    "                    \"doc_id\": parent_id,\n",
    "                    \"type\": \"parent\",\n",
    "                    \"source\": source_name,\n",
    "                    \"urls\": specific_urls, \n",
    "                })\n",
    "                final_parent_doc = Document(page_content=clean_text, metadata=parent_meta)\n",
    "                all_parent_docs.append(final_parent_doc)\n",
    "                # Generate Child Chunk dari Parent Chunk\n",
    "                child_texts = self.child_splitter.split_text(clean_text)\n",
    "                for c_text in child_texts:\n",
    "                    child_meta = {\n",
    "                        \"parent_id\": parent_id,\n",
    "                        \"type\": \"child\",\n",
    "                        \"source\": source_name,\n",
    "                    }\n",
    "                    child_doc = Document(page_content=c_text, metadata=child_meta)\n",
    "                    all_child_docs.append(child_doc)\n",
    "\n",
    "        return {\"parents\": all_parent_docs,\n",
    "                \"children\": all_child_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_embeddings(provider: str = EMBEDDING_PROVIDER, model_id: str = EMBEDDING_MODEL_ID) -> Embeddings:\n",
    "    if provider == \"aws\":\n",
    "        return BedrockEmbeddings(model_id=model_id,     \n",
    "                                region_name=os.getenv(\"AWS_REGION\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Provider embedding tidak didukung\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "def encode(doc: Document) -> bytes:\n",
    "    return dumps(doc).encode(\"utf-8\")\n",
    "\n",
    "def decode(data: bytes) -> Document:\n",
    "    if isinstance(data, str):\n",
    "        data = data.encode(\"utf-8\")\n",
    "    return loads(data.decode(\"utf-8\"))\n",
    "\n",
    "def encode_key(key: Any) -> str:\n",
    "    if isinstance(key, bytes):\n",
    "        return key.decode(\"utf-8\")\n",
    "    return str(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class VectorStore:\n",
    "    @staticmethod\n",
    "    def get_vector_store(embedding_model: Embeddings, collection_name: str = COLLECTION_NAME, persist_directory: str = PERSIST_DIR):\n",
    "        return Chroma(embedding_function=embedding_model, \n",
    "                    persist_directory=persist_directory,\n",
    "                    collection_name=collection_name,\n",
    "                    collection_metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain.storage import EncoderBackedStore\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DocStore:\n",
    "    @staticmethod\n",
    "    def get_doc_store(collection_name: str = COLLECTION_NAME, redis_url: str = REDIS_URL):\n",
    "        namespace = f\"docstore_{collection_name}\"\n",
    "        try:\n",
    "            logger.info(f\"Connecting to Redis DocStore {redis_url}, Namespace: {namespace})\")\n",
    "            raw_store = RedisStore(redis_url=redis_url, namespace=namespace)\n",
    "            return EncoderBackedStore(store=raw_store,\n",
    "                                    key_encoder=encode_key,\n",
    "                                    value_serializer=encode,\n",
    "                                    value_deserializer=decode)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal koneksi ke Redis {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from typing import Set, List\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_stopwords(path: str) -> Set[str]:\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                stopwords = {line.strip().lower() for line in f if line.strip()}\n",
    "            logger.info(\"Stopwords berhasil di load\")\n",
    "            return stopwords\n",
    "        else:\n",
    "            logger.warning(f\"Stopword file tidak ditemukan {path}\")\n",
    "            return set()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gagal load stopwords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def preprocess_text(text: str, stopwords: Set[str]) -> List[str]:\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"[a-z0-9]+(?:-[a-z0-9]+)*\", text)\n",
    "    if stopwords:\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent-Child Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import pickle\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from typing import List, Dict\n",
    "from functools import partial\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ParentChildIndexer:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = get_embeddings()\n",
    "        self.collection_name = COLLECTION_NAME\n",
    "        self.vector_store = VectorStore.get_vector_store(\n",
    "            embedding_model=self.embedding_model,\n",
    "            collection_name=self.collection_name\n",
    "        )\n",
    "        self.doc_store = DocStore.get_doc_store(collection_name=self.collection_name)\n",
    "\n",
    "    def index_documents(self, split_result: Dict[str, List[Document]]):\n",
    "        parent_docs = split_result.get(\"parents\", [])\n",
    "        child_docs = split_result.get(\"children\", [])\n",
    "        logger.info(f\"Indexing {len(parent_docs)} Parents & {len(child_docs)} Child\")\n",
    "        # SIMPAN PARENTS KE REDIS\n",
    "        if parent_docs:\n",
    "            try:\n",
    "                parent_key_value_pairs = []\n",
    "                for doc in parent_docs:\n",
    "                    doc_id = doc.metadata.get(\"doc_id\") or str(uuid.uuid4())\n",
    "                    doc.metadata[\"doc_id\"] = doc_id\n",
    "                    parent_key_value_pairs.append((doc_id, doc))\n",
    "                self.doc_store.mset(parent_key_value_pairs)\n",
    "                logger.info(f\"Berhasil menyimpan {len(parent_docs)} Parent Chunk ke Redis.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error simpan Parent Chunk ke Redis: {e}\")\n",
    "                raise e\n",
    "        # SIMPAN CHILDREN KE CHROMA\n",
    "        if child_docs:\n",
    "            try:\n",
    "                valid_children = [d for d in child_docs if \"parent_id\" in d.metadata]\n",
    "                self.vector_store.add_documents(valid_children)\n",
    "                logger.info(f\"Berhasil menyimpan {len(valid_children)} Child Chunk ke ChromaDB.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error simpan Children Chunk ke Chroma: {e}\")\n",
    "                raise e\n",
    "        return {\"Parents indexed\": len(parent_docs), \"Children indexed\": len(child_docs)}\n",
    "\n",
    "class BM25Indexer:\n",
    "    def __init__(self):\n",
    "        self.vector_store = VectorStore.get_vector_store(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            embedding_model=get_embeddings()\n",
    "        )\n",
    "    def build_and_save_index(self):\n",
    "        # 1. Load stopwords\n",
    "        stopwords = load_stopwords(STOPWORDS_PATH)\n",
    "        # 2. Fetch documents child dari vectordb\n",
    "        try:\n",
    "            result = self.vector_store.get(\n",
    "                where={\"type\": \"child\"},\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal fetch data dari Chroma: {e}\")\n",
    "            raise e\n",
    "        raw_docs = result.get(\"documents\", [])\n",
    "        raw_metadatas = result.get(\"metadatas\", [])\n",
    "        if not raw_docs:\n",
    "            logger.warning(\"Tidak ditemukan dokumen 'child' di Chroma\")\n",
    "            return\n",
    "        logger.info(f\"Indexing {len(raw_docs)} dokumen\")\n",
    "        # 3. Convert ke LangChain Document\n",
    "        documents = [\n",
    "            Document(page_content=text, metadata=meta)\n",
    "            for text, meta in zip(raw_docs, raw_metadatas)\n",
    "        ]\n",
    "        # 4. Build BM25 retriever\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents, preprocess_func=partial(preprocess_text, stopwords=stopwords))\n",
    "        # 5. Save index\n",
    "        try:\n",
    "            with open(BM25_INDEX_PATH, \"wb\") as f:\n",
    "                pickle.dump(bm25_retriever, f)\n",
    "            logger.info(f\"BM25 Index berhasil disimpan di: {BM25_INDEX_PATH}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal menyimpan file pickle: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run_ingestion_pipeline(folder_path: str):\n",
    "    print(\"Loading Data\")\n",
    "    loader = PDFDirectoryLoader()\n",
    "    raw_docs = loader.load(folder_path)\n",
    "    print(\"Splitting Dokumen\")\n",
    "    splitter = ParentChildSplitter(\n",
    "        parent_chunk_size=PARENT_CHUNK_SIZE,\n",
    "        parent_chunk_overlap=PARENT_CHUNK_OVERLAP,\n",
    "        child_chunk_size=CHILD_CHUNK_SIZE,\n",
    "        child_chunk_overlap=CHILD_CHUNK_OVERLAP\n",
    "    )\n",
    "    split_result = splitter.split_documents(raw_docs)\n",
    "    print(\"Indexing Document Chunks\")\n",
    "    vector_indexer = ParentChildIndexer()\n",
    "    result = vector_indexer.index_documents(split_result)\n",
    "    print(f\"{result}\")\n",
    "    print(\"Building BM25 Index\")\n",
    "    keyword_indexer = BM25Indexer()\n",
    "    keyword_indexer.build_and_save_index()\n",
    "    print(\"Ingestion DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Splitting Dokumen\n",
      "Indexing Document Chunks\n",
      "{'Parents indexed': 90, 'Children indexed': 631}\n",
      "Building BM25 Index\n",
      "Ingestion DONE\n"
     ]
    }
   ],
   "source": [
    "run_ingestion_pipeline(\"MajaAI_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatible Langflow Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LangflowCompatibleLoader:\n",
    "    def __init__(self, recursive: bool = True):\n",
    "        self.recursive = recursive\n",
    "\n",
    "    def load(self, directory_path: str) -> List[Document]:\n",
    "        documents = []\n",
    "        # Pola glob recursive ala Langflow\n",
    "        search_pattern = os.path.join(directory_path, \"**/*.pdf\") if self.recursive else os.path.join(directory_path, \"*.pdf\")\n",
    "        \n",
    "        # recursive=True di glob hanya bekerja jika pola mengandung \"**\"\n",
    "        pdf_files = glob.glob(search_pattern, recursive=self.recursive)\n",
    "        \n",
    "        if not pdf_files:\n",
    "            logger.warning(f\"Tidak ditemukan file PDF di: {directory_path}\")\n",
    "            return []\n",
    "\n",
    "        logger.info(f\"Ditemukan {len(pdf_files)} file PDF.\")\n",
    "\n",
    "        for file_path in pdf_files:\n",
    "            try:\n",
    "                # Langflow menggunakan PyMuPDFLoader standar tanpa pre-cleaning \\x00\n",
    "                loader = PyMuPDFLoader(file_path)\n",
    "                docs = loader.load()\n",
    "                \n",
    "                for doc in docs:\n",
    "                    # Metadata standar Langflow\n",
    "                    doc.metadata[\"source\"] = file_path  # Langflow pakai 'source'\n",
    "                    doc.metadata[\"file_path\"] = file_path\n",
    "                \n",
    "                documents.extend(docs)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {file_path}: {e}\")\n",
    "                \n",
    "        logger.info(f\"Total {len(documents)} halaman berhasil dimuat.\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LangflowCompatibleSplitter:\n",
    "    def __init__(self, parent_chunk_size: int, parent_chunk_overlap: int, child_chunk_size: int, child_chunk_overlap: int):\n",
    "        self.parent_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=parent_chunk_size,\n",
    "            chunk_overlap=parent_chunk_overlap,\n",
    "            separators=[\"\\n\", \". \", \", \", \" \", \"\"]\n",
    "        )\n",
    "        self.child_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=child_chunk_size,\n",
    "            chunk_overlap=child_chunk_overlap,\n",
    "            # Langflow child splitter punya prioritas \\n\\n meski sudah di-clean\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"] \n",
    "        )\n",
    "\n",
    "    def _preprocess_text(self, chunk_text: str) -> Tuple[str, List[str]]:\n",
    "        # 1. URL handling (Langflow code: https:// -> [LINK])\n",
    "        # Note: Code Langflow Anda pakai 'https://' (tanpa ?). \n",
    "        # Code LangChain Anda pakai 'https?://'. \n",
    "        # Kita pakai versi LangChain (lebih robust) atau Langflow (lebih strict).\n",
    "        # Di sini saya pakai versi LangChain Anda karena lebih aman, impact ke akurasi minim.\n",
    "        urls = re.findall(r'https?://\\S+', chunk_text)\n",
    "        text_clean = re.sub(r'https?://\\S+', '[LINK]', chunk_text)\n",
    "\n",
    "        # 2. Character removal (SAMA PERSIS)\n",
    "        text_clean = re.sub(r'[^\\w\\s\\[\\],.:()%=+\\-/]', '', text_clean)\n",
    "\n",
    "        # 3. Newline cleanup (SAMA PERSIS)\n",
    "        # Ini mengubah \\n\\n menjadi \\n\n",
    "        text_clean = re.sub(r'\\n{2,}', '\\n', text_clean)\n",
    "        text_clean = re.sub(r'[ ]{2,}', ' ', text_clean)\n",
    "        text_clean = re.sub(r'\\n\\s*\\n+', '\\n', text_clean)\n",
    "        \n",
    "        return text_clean.strip(), urls\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> Dict[str, List[Document]]:\n",
    "        docs_by_source = defaultdict(list)\n",
    "        for doc in documents:\n",
    "            # Langflow prefer metadata \"source\", fallback ke \"file_path\"\n",
    "            source_key = doc.metadata.get(\"source\") or doc.metadata.get(\"file_path\")\n",
    "            docs_by_source[source_key].append(doc)\n",
    "\n",
    "        all_parent_docs = []\n",
    "        all_child_docs = []\n",
    "\n",
    "        logger.info(f\"Processing split for {len(docs_by_source)} unique sources\")\n",
    "\n",
    "        for source_name, doc_group in docs_by_source.items():\n",
    "            # 1. MERGE PAGES (SAMA PERSIS)\n",
    "            combined_text = \"\\n\".join([d.page_content for d in doc_group])\n",
    "            base_metadata = doc_group[0].metadata.copy() if doc_group else {}\n",
    "            \n",
    "            combined_doc = Document(page_content=combined_text, metadata=base_metadata)\n",
    "\n",
    "            # 2. PARENT SPLIT (Raw Text -> Raw Parent Chunks)\n",
    "            parent_chunks = self.parent_splitter.split_documents([combined_doc])\n",
    "\n",
    "            for p_doc in parent_chunks:\n",
    "                # 3. PREPROCESS (Raw Parent -> Clean Parent)\n",
    "                clean_text, specific_urls = self._preprocess_text(p_doc.page_content)\n",
    "                \n",
    "                # Skip jika kosong setelah dibersihkan\n",
    "                if not clean_text:\n",
    "                    continue\n",
    "\n",
    "                parent_id = str(uuid.uuid4())\n",
    "                \n",
    "                # Metadata update\n",
    "                parent_meta = p_doc.metadata.copy()\n",
    "                parent_meta.update({\n",
    "                    \"doc_id\": parent_id,\n",
    "                    \"type\": \"parent\",\n",
    "                    \"source\": source_name,\n",
    "                    \"urls\": specific_urls, \n",
    "                })\n",
    "\n",
    "                final_parent_doc = Document(page_content=clean_text, metadata=parent_meta)\n",
    "                all_parent_docs.append(final_parent_doc)\n",
    "\n",
    "                # 4. CHILD SPLIT (Clean Parent -> Clean Children)\n",
    "                # Splitter child menerima teks yang SUDAH bersih (tanpa \\n\\n)\n",
    "                child_texts = self.child_splitter.split_text(clean_text)\n",
    "                \n",
    "                for c_text in child_texts:\n",
    "                    if not c_text.strip(): continue\n",
    "                    \n",
    "                    child_meta = {\n",
    "                        \"parent_id\": parent_id,\n",
    "                        \"type\": \"child\",\n",
    "                        \"source\": source_name,\n",
    "                        # Opsional: Child tidak wajib punya source full path untuk hemat redis, \n",
    "                        # tapi untuk konsistensi kita simpan.\n",
    "                    }\n",
    "                    child_doc = Document(page_content=c_text, metadata=child_meta)\n",
    "                    all_child_docs.append(child_doc)\n",
    "\n",
    "        return {\"parents\": all_parent_docs, \"children\": all_child_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import uuid\n",
    "import time\n",
    "import chromadb\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSerializable, RunnableConfig\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain.storage import EncoderBackedStore\n",
    "from langchain_core.load import dumps, loads\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER: DOCUMENT ENCODER (Sama Persis) ---\n",
    "class DocumentEncoder:\n",
    "    def encode(self, doc: Document) -> bytes:\n",
    "        return dumps(doc).encode(\"utf-8\")\n",
    "\n",
    "    def decode(self, data: bytes) -> Document:\n",
    "        return loads(data.decode(\"utf-8\"))\n",
    "\n",
    "# --- HELPER: FACTORY STORE ---\n",
    "class StoreFactory:\n",
    "    \"\"\"\n",
    "    Factory untuk membuat koneksi Redis & Chroma yang konsisten.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_doc_store(redis_url: str, collection_name: str):\n",
    "        docstore_namespace = f\"docstore_{collection_name}\"\n",
    "        redis_raw = RedisStore(redis_url=redis_url, namespace=docstore_namespace)\n",
    "        encoder = DocumentEncoder()\n",
    "        \n",
    "        # Replikasi Logika Langflow: Helper key encoder\n",
    "        def encode_key(key):\n",
    "            return str(key.decode(\"utf-8\") if isinstance(key, bytes) else key)\n",
    "\n",
    "        return EncoderBackedStore(\n",
    "            store=redis_raw,\n",
    "            key_encoder=encode_key,\n",
    "            value_serializer=encoder.encode,\n",
    "            value_deserializer=encoder.decode,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_vector_store(embedding: Embeddings, collection_name: str):\n",
    "        client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "        return Chroma(\n",
    "            client=client,\n",
    "            embedding_function=embedding,\n",
    "            collection_name=collection_name,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"} \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangflowCompatibleIndexer:\n",
    "    \"\"\"\n",
    "    Replikasi logika 'HierarchicalStore' Langflow.\n",
    "    Tugas: Menjamin Metadata (doc_id, type, parent_id) terisi konsisten.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding: Embeddings, redis_url: str, collection_name: str = \"langflow\"):\n",
    "        self.doc_store = StoreFactory.get_doc_store(redis_url, collection_name)\n",
    "        self.vector_store = StoreFactory.get_vector_store(embedding, collection_name)\n",
    "\n",
    "    def index(self, parents: List[Document], children: List[Document]):\n",
    "        logger.info(f\"Mulai Indexing: {len(parents)} Parents, {len(children)} Children\")\n",
    "        \n",
    "        # 1. PROSES PARENT (Replikasi logic Langflow)\n",
    "        # Pastikan Parent punya ID di metadata\n",
    "        processed_parents = []\n",
    "        for p in parents:\n",
    "            if \"doc_id\" not in p.metadata:\n",
    "                p.metadata[\"doc_id\"] = str(uuid.uuid4())\n",
    "            processed_parents.append(p)\n",
    "\n",
    "        # 2. PROSES CHILDREN (CRITICAL POINT)\n",
    "        # Langflow secara eksplisit menyuntikkan 'type': 'child'\n",
    "        processed_children = []\n",
    "        for c in children:\n",
    "            # Pastikan punya doc_id\n",
    "            if \"doc_id\" not in c.metadata:\n",
    "                c.metadata[\"doc_id\"] = str(uuid.uuid4())\n",
    "            \n",
    "            # [PENTING] Enforce metadata type='child'\n",
    "            # Tanpa ini, filter={\"type\": \"child\"} di search akan gagal.\n",
    "            c.metadata[\"type\"] = \"child\"\n",
    "            \n",
    "            # Validasi link ke parent (Opsional tapi disarankan)\n",
    "            if \"parent_id\" in c.metadata:\n",
    "                processed_children.append(c)\n",
    "            else:\n",
    "                logger.warning(f\"Child dibuang (missing parent_id): {c.page_content[:30]}...\")\n",
    "\n",
    "        # 3. SIMPAN PARENT KE REDIS\n",
    "        if processed_parents:\n",
    "            try:\n",
    "                # Map doc_id -> Document Object\n",
    "                parent_key_val = [(p.metadata[\"doc_id\"], p) for p in processed_parents]\n",
    "                self.doc_store.mset(parent_key_val)\n",
    "                logger.info(f\"Sukses menyimpan {len(parent_key_val)} Parent Docs ke Redis\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Gagal simpan Redis: {e}\")\n",
    "                raise e\n",
    "\n",
    "        # 4. SIMPAN CHILDREN KE CHROMA\n",
    "        if processed_children:\n",
    "            try:\n",
    "                self.vector_store.add_documents(processed_children)\n",
    "                logger.info(f\"Sukses menyimpan {len(processed_children)} Child Chunks ke Chroma\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Gagal simpan Chroma: {e}\")\n",
    "                raise e\n",
    "        return {\"status\": \"Success\", \"parents\": len(processed_parents), \"children\": len(processed_children)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:--- 1. Menginisialisasi Komponen ---\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v2/auth/identity \"HTTP/1.1 200 OK\"\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:--- 2. Memuat Dokumen dari MajaAI_Data ---\n",
      "INFO:__main__:Ditemukan 45 file PDF.\n",
      "INFO:__main__:Total 72 halaman berhasil dimuat.\n",
      "INFO:__main__:Berhasil memuat 72 halaman raw dari PDF.\n",
      "INFO:__main__:--- 3. Melakukan Splitting Parent-Child ---\n",
      "INFO:__main__:Processing split for 45 unique sources\n",
      "INFO:__main__:Splitting Selesai. Parents: 90, Children: 631\n",
      "INFO:__main__:--- 4. Menyimpan ke Vector DB & Redis ---\n",
      "INFO:__main__:Mulai Indexing: 90 Parents, 631 Children\n",
      "INFO:__main__:Sukses menyimpan 90 Parent Docs ke Redis\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v2/pre-flight-checks \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/upsert \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Sukses menyimpan 631 Child Chunks ke Chroma\n",
      "INFO:__main__:Indexing Sukses! Status: {'status': 'Success', 'parents': 90, 'children': 631}\n"
     ]
    }
   ],
   "source": [
    "# --- KONFIGURASI ---\n",
    "SOURCE_DIRECTORY = \"MajaAI_Data\"  # Folder tempat menaruh file PDF\n",
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "# CHROMA_DIR = \"./data/parent-child-chunk-langflow-compatible\"\n",
    "COLLECTION = \"parent-child-chunk-langflow-v5\"\n",
    "AWS_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "# Setup Logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run_indexing_pipeline():\n",
    "    # 1. INISIALISASI KOMPONEN\n",
    "    logger.info(\"--- 1. Menginisialisasi Komponen ---\")\n",
    "    \n",
    "    # Ambil embedding model (tanpa wrapper Embeddings())\n",
    "    embedding_model = get_embeddings(\"aws\", AWS_MODEL_ID)\n",
    "    \n",
    "    # Siapkan Loader\n",
    "    pdf_loader = LangflowCompatibleLoader()\n",
    "    \n",
    "    # Siapkan Splitter\n",
    "    splitter = LangflowCompatibleSplitter(\n",
    "        parent_chunk_size=2400,\n",
    "        parent_chunk_overlap=260,\n",
    "        child_chunk_size=300,\n",
    "        child_chunk_overlap=60\n",
    "    )\n",
    "    \n",
    "    # Siapkan Indexer\n",
    "    indexer = LangflowCompatibleIndexer(\n",
    "        embedding=embedding_model,\n",
    "        redis_url=REDIS_URL,\n",
    "        collection_name=COLLECTION\n",
    "    )\n",
    "\n",
    "    # 2. LOAD DOKUMEN (Tahap Integrasi Loader)\n",
    "    logger.info(f\"--- 2. Memuat Dokumen dari {SOURCE_DIRECTORY} ---\")\n",
    "    \n",
    "    # Pastikan folder ada\n",
    "    if not os.path.exists(SOURCE_DIRECTORY):\n",
    "        os.makedirs(SOURCE_DIRECTORY)\n",
    "        logger.warning(f\"Folder '{SOURCE_DIRECTORY}' baru saja dibuat. Masukkan file PDF ke dalamnya lalu jalankan ulang.\")\n",
    "        return\n",
    "\n",
    "    # Load Raw Documents\n",
    "    raw_documents = pdf_loader.load(SOURCE_DIRECTORY)\n",
    "    \n",
    "    if not raw_documents:\n",
    "        logger.warning(\"Tidak ada dokumen yang dimuat. Stopping pipeline.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Berhasil memuat {len(raw_documents)} halaman raw dari PDF.\")\n",
    "\n",
    "    # 3. SPLITTING (Raw Docs -> Parent & Child Chunks)\n",
    "    logger.info(\"--- 3. Melakukan Splitting Parent-Child ---\")\n",
    "    \n",
    "    # Splitter akan menggunakan metadata['file_path'] dari Loader \n",
    "    # untuk menggabungkan halaman dari file yang sama sebelum memecahnya.\n",
    "    split_result = splitter.split_documents(raw_documents)\n",
    "    \n",
    "    parent_docs = split_result[\"parents\"]\n",
    "    child_docs = split_result[\"children\"]\n",
    "    \n",
    "    logger.info(f\"Splitting Selesai. Parents: {len(parent_docs)}, Children: {len(child_docs)}\")\n",
    "\n",
    "    # 4. INDEXING (Simpan ke DB)\n",
    "    logger.info(\"--- 4. Menyimpan ke Vector DB & Redis ---\")\n",
    "    \n",
    "    try:\n",
    "        status = indexer.index(parent_docs, child_docs)\n",
    "        logger.info(f\"Indexing Sukses! Status: {status}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Indexing Gagal: {e}\")\n",
    "\n",
    "run_indexing_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangflowCompatibleRetriever(RunnableSerializable):\n",
    "    \"\"\"\n",
    "    Replikasi logika 'Parent_Child_Search' Langflow.\n",
    "    \"\"\"\n",
    "    collection_name: str = \"parent-child-chunk-langflow-v5\"\n",
    "    redis_url: str = \"redis://localhost:6379\"\n",
    "    max_results: int = 2\n",
    "    \n",
    "    # Internal\n",
    "    _doc_store: Any = PrivateAttr()\n",
    "    _vector_store: Any = PrivateAttr()\n",
    "    _embedding: Embeddings = PrivateAttr()\n",
    "\n",
    "    def __init__(self, embedding: Embeddings, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._embedding = embedding\n",
    "        # Inisialisasi koneksi\n",
    "        self._doc_store = StoreFactory.get_doc_store(self.redis_url, self.collection_name)\n",
    "        self._vector_store = StoreFactory.get_vector_store(self._embedding, self.collection_name)\n",
    "\n",
    "    def invoke(self, query: str, config: Optional[RunnableConfig] = None, **kwargs) -> List[Document]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. SEARCH CHILD CHUNKS\n",
    "        # Langflow Logic: Ambil kandidat 2x max_results\n",
    "        k_candidates = self.max_results * 2\n",
    "        \n",
    "        try:\n",
    "            child_results = self._vector_store.similarity_search_with_relevance_scores(\n",
    "                query=query,\n",
    "                k=k_candidates,\n",
    "                filter={\"type\": \"child\"}, # Filter ini hanya berhasil jika Indexer menambahkan tag 'child'\n",
    "                score_threshold=0.01      # Threshold default Langflow\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Vector Search Error: {e}\")\n",
    "            return []\n",
    "\n",
    "        if not child_results:\n",
    "            logger.warning(\"No results found in vector store.\")\n",
    "            return []\n",
    "\n",
    "        # 2. DEDUPLIKASI & LINKING (Logika Langflow)\n",
    "        # Mengambil hanya child skor tertinggi untuk setiap parent_id unik\n",
    "        parent_id_map = {}\n",
    "        unique_parent_ids = []\n",
    "\n",
    "        for child, score in child_results:\n",
    "            pid = child.metadata.get(\"parent_id\")\n",
    "            if not pid:\n",
    "                continue\n",
    "            \n",
    "            # Langflow logic: \"if parent_id not in parent_id_to_data\"\n",
    "            # Artinya: First come (skor tertinggi), first served. Sisanya diabaikan.\n",
    "            if pid not in parent_id_map:\n",
    "                parent_id_map[pid] = {\n",
    "                    \"score\": score,\n",
    "                    \"matched_child_content\": child.page_content\n",
    "                }\n",
    "                unique_parent_ids.append(pid)\n",
    "\n",
    "        # 3. FETCH PARENTS FROM REDIS\n",
    "        if not unique_parent_ids:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            parent_docs = self._doc_store.mget(unique_parent_ids)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Redis Fetch Error: {e}\")\n",
    "            return []\n",
    "\n",
    "        # 4. FORMATTING & RANKING\n",
    "        final_results = []\n",
    "        for pid, p_doc in zip(unique_parent_ids, parent_docs):\n",
    "            if p_doc:\n",
    "                info = parent_id_map[pid]\n",
    "                \n",
    "                # Inject Score & Child Content ke Metadata Parent (Sama seperti output Langflow)\n",
    "                p_doc.metadata[\"retrieval_score\"] = round(info[\"score\"], 2) # Langflow melakukan rounding\n",
    "                p_doc.metadata[\"matched_child_content\"] = info[\"matched_child_content\"]\n",
    "                \n",
    "                final_results.append(p_doc)\n",
    "\n",
    "        # Sort descending by score\n",
    "        final_results.sort(key=lambda x: x.metadata.get(\"retrieval_score\", 0), reverse=True)\n",
    "        \n",
    "        # Potong sesuai max_results\n",
    "        return final_results[:self.max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v2/auth/identity \"HTTP/1.1 200 OK\"\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# --- 3. SEARCHING ---\n",
    "retriever = LangflowCompatibleRetriever(\n",
    "    embedding=get_embeddings(\"aws\", \"amazon.titan-embed-text-v2:0\"),\n",
    "    redis_url=REDIS_URL,\n",
    "    collection_name=COLLECTION,\n",
    "    max_results=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.85\n",
      "File Path: MajaAI_Data\\031_Daftar layanan publik kategori sosial.pdf\n",
      "Parent Content: Kategori / topik\n",
      ": Daftar layanan publik kategori sosial \n",
      "Pertanyaan utama / intent \n",
      ": Apa saja laya...\n",
      "Matched Child: Kategori / topik\n",
      ": Daftar layanan publik kategori sosial \n",
      "Pertanyaan utama / intent \n",
      ": Apa saja laya...\n",
      "--------------------\n",
      "Score: 0.68\n",
      "File Path: MajaAI_Data\\034_Daftar layanan Pemerintah dan desa.pdf\n",
      "Parent Content: Layanan pemerintah dan desa adalah pelayanan publik yang diberikan oleh pemerintah daerah tingkat \n",
      "k...\n",
      "Matched Child: di Jawa Timur. Pelayanannya meliputi pengurusan administrasi kependudukan, retribusi daerah, dan \n",
      "be...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Apa saja layanan sosial yang tersedia di Majadigi?\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"Score: {doc.metadata['retrieval_score']}\")\n",
    "    print(f\"File Path: {doc.metadata['file_path']}\")\n",
    "    print(f\"Parent Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"Matched Child: {doc.metadata['matched_child_content'][:100]}...\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAs Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Documents\\AI Engineer Portofolio\\LLM\\langchain-course\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import LLMContextRecall, ContextPrecision\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from datasets import Dataset\n",
    "from langchain_aws import BedrockEmbeddings, BedrockLLM\n",
    "from langchain_aws.chat_models import ChatBedrock\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def format_docs_to_string(docs: List[Document]) -> str:\n",
    "    return \"\\n\".join(doc.page_content for doc in docs) #Harusnya \\n\n",
    "\n",
    "def extract_urls_from_docs(docs: List[Document]) -> List[str]:\n",
    "    all_urls = []\n",
    "    for doc in docs:\n",
    "        urls = doc.metadata.get(\"urls\", [])\n",
    "        if isinstance(urls, str):\n",
    "            urls = [urls]\n",
    "        all_urls.extend(urls)\n",
    "    return sorted(set(all_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_retriever(retriever):\n",
    "    def _run(input):\n",
    "        start = time.time()\n",
    "        docs = retriever.invoke(input)\n",
    "        end = time.time()\n",
    "        return {\n",
    "            \"docs\": docs,\n",
    "            \"retriever_latency\": end - start\n",
    "        }\n",
    "    return RunnableLambda(_run)\n",
    "\n",
    "timed_retriever = time_retriever(retriever)\n",
    "\n",
    "retrieval = (\n",
    "    itemgetter(\"question\")\n",
    "    | timed_retriever\n",
    ")\n",
    "\n",
    "RAG_retriever_eval = (\n",
    "    retrieval\n",
    "    | RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs_to_string(x[\"docs\"]),\n",
    "        url=lambda x: extract_urls_from_docs(x[\"docs\"])\n",
    "    )\n",
    "    | RunnableParallel({\n",
    "        \"context\": itemgetter(\"docs\"),\n",
    "        \"retriever_latency\": itemgetter(\"retriever_latency\"),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20852\\4087963755.py:3: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm_ragas = LangchainLLMWrapper(ChatBedrock(model='apac.amazon.nova-pro-v1:0', temperature=0))\n"
     ]
    }
   ],
   "source": [
    "df_eval = pd.read_csv(\"weekly backlog - QnA Maja.AI (2).csv\")\n",
    "total_data = len(df_eval) #60\n",
    "llm_ragas = LangchainLLMWrapper(ChatBedrock(model='apac.amazon.nova-pro-v1:0', temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_context(question:str):\n",
    "    RAG_output = RAG_retriever_eval.invoke({\"question\": question})\n",
    "    context = RAG_output[\"context\"]\n",
    "    contexts = [doc.page_content for doc in context]\n",
    "    latency = RAG_output[\"retriever_latency\"]\n",
    "    return contexts, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 60 Data:   0%|          | 0/60 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:   2%|         | 1/60 [00:00<00:53,  1.10it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:   3%|         | 2/60 [00:01<00:31,  1.82it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:   5%|         | 3/60 [00:01<00:24,  2.35it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:   7%|         | 4/60 [00:01<00:20,  2.80it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:   8%|         | 5/60 [00:01<00:17,  3.17it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  10%|         | 6/60 [00:02<00:16,  3.25it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  12%|        | 7/60 [00:02<00:16,  3.18it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  13%|        | 8/60 [00:02<00:15,  3.46it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  15%|        | 9/60 [00:03<00:14,  3.57it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  17%|        | 10/60 [00:04<00:38,  1.29it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  18%|        | 11/60 [00:05<00:29,  1.66it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  20%|        | 12/60 [00:05<00:24,  1.94it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  22%|       | 13/60 [00:05<00:20,  2.28it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  23%|       | 14/60 [00:06<00:17,  2.64it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  25%|       | 15/60 [00:06<00:15,  2.81it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  27%|       | 16/60 [00:06<00:14,  3.07it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  28%|       | 17/60 [00:06<00:12,  3.34it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  30%|       | 18/60 [00:07<00:12,  3.50it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  32%|      | 19/60 [00:07<00:11,  3.61it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  33%|      | 20/60 [00:07<00:11,  3.54it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  35%|      | 21/60 [00:07<00:10,  3.70it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  37%|      | 22/60 [00:08<00:09,  3.85it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  38%|      | 23/60 [00:08<00:09,  3.97it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  40%|      | 24/60 [00:08<00:08,  4.08it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  42%|     | 25/60 [00:08<00:08,  4.18it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  43%|     | 26/60 [00:09<00:08,  4.23it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  45%|     | 27/60 [00:09<00:07,  4.15it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  47%|     | 28/60 [00:09<00:07,  4.19it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  48%|     | 29/60 [00:09<00:07,  3.96it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  50%|     | 30/60 [00:10<00:07,  3.78it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  52%|    | 31/60 [00:10<00:07,  3.69it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  53%|    | 32/60 [00:10<00:07,  3.62it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  55%|    | 33/60 [00:10<00:07,  3.63it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  57%|    | 34/60 [00:11<00:07,  3.62it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  58%|    | 35/60 [00:11<00:07,  3.19it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  60%|    | 36/60 [00:12<00:13,  1.72it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  62%|   | 37/60 [00:13<00:11,  2.02it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  63%|   | 38/60 [00:13<00:09,  2.29it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  65%|   | 39/60 [00:13<00:08,  2.60it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  67%|   | 40/60 [00:13<00:07,  2.83it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  68%|   | 41/60 [00:14<00:06,  2.97it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  70%|   | 42/60 [00:14<00:06,  2.86it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  72%|  | 43/60 [00:14<00:05,  2.97it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  73%|  | 44/60 [00:15<00:05,  3.13it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  75%|  | 45/60 [00:15<00:04,  3.28it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  77%|  | 46/60 [00:15<00:04,  3.29it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  78%|  | 47/60 [00:16<00:03,  3.40it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  80%|  | 48/60 [00:16<00:03,  3.47it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  82%| | 49/60 [00:16<00:03,  3.48it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  83%| | 50/60 [00:16<00:02,  3.55it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  85%| | 51/60 [00:17<00:02,  3.42it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  87%| | 52/60 [00:17<00:02,  3.44it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  88%| | 53/60 [00:17<00:02,  3.43it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  90%| | 54/60 [00:18<00:01,  3.50it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  92%|| 55/60 [00:18<00:01,  3.45it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  93%|| 56/60 [00:18<00:01,  3.48it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  95%|| 57/60 [00:18<00:00,  3.44it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  97%|| 58/60 [00:19<00:00,  3.63it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data:  98%|| 59/60 [00:19<00:00,  3.56it/s]INFO:httpx:HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/9f434249-4421-41a2-bdb4-e9390ee93b1e/query \"HTTP/1.1 200 OK\"\n",
      "Loading 60 Data: 100%|| 60/60 [00:19<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate RAGAs pada 60 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: bedrock-runtime.ap-northeast-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: bedrock-runtime.ap-northeast-1.amazonaws.com. Connection pool size: 10\n",
      "Evaluating:   1%|          | 1/120 [00:18<36:48, 18.55s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:   2%|         | 2/120 [00:47<48:06, 24.46s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:   8%|         | 9/120 [00:55<08:24,  4.55s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:   9%|         | 11/120 [01:30<14:21,  7.91s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  18%|        | 21/120 [01:38<05:24,  3.28s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  19%|        | 23/120 [01:58<07:06,  4.40s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  25%|       | 30/120 [02:15<05:17,  3.53s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: bedrock-runtime.ap-northeast-1.amazonaws.com. Connection pool size: 10\n",
      "Evaluating:  28%|       | 34/120 [02:22<04:22,  3.05s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: bedrock-runtime.ap-northeast-1.amazonaws.com. Connection pool size: 10\n",
      "Evaluating:  29%|       | 35/120 [02:50<07:20,  5.19s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: bedrock-runtime.ap-northeast-1.amazonaws.com. Connection pool size: 10\n",
      "Evaluating:  36%|      | 43/120 [03:05<04:31,  3.53s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  38%|      | 46/120 [03:32<05:46,  4.68s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  44%|     | 53/120 [03:46<03:58,  3.57s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  48%|     | 57/120 [04:06<04:09,  3.96s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  52%|    | 63/120 [04:09<02:35,  2.72s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  53%|    | 64/120 [04:27<03:37,  3.88s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  57%|    | 68/120 [04:30<02:34,  2.97s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  57%|    | 69/120 [04:49<03:48,  4.48s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  61%|    | 73/120 [05:09<03:40,  4.70s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  66%|   | 79/120 [05:32<02:56,  4.30s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  70%|   | 84/120 [05:52<02:32,  4.24s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: bedrock-runtime.ap-northeast-1.amazonaws.com. Connection pool size: 10\n",
      "Evaluating:  74%|  | 89/120 [05:58<01:40,  3.25s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  77%|  | 92/120 [06:30<02:36,  5.61s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  82%| | 99/120 [06:44<01:18,  3.75s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  84%| | 101/120 [06:47<01:04,  3.42s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  85%| | 102/120 [06:53<01:06,  3.67s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  87%| | 104/120 [07:06<01:09,  4.32s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  88%| | 105/120 [07:26<01:39,  6.63s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  93%|| 112/120 [07:30<00:24,  3.02s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  94%|| 113/120 [07:38<00:24,  3.52s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating:  99%|| 119/120 [07:53<00:02,  2.82s/it]INFO:langchain_aws.chat_models.bedrock_converse:Using Bedrock Converse API to generate response\n",
      "Evaluating: 100%|| 120/120 [07:56<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hasil Evaluasi RAGAs ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apa perbedaan pengurusan e-KTP online dan offl...</td>\n",
       "      <td>[Kategori\\n: Dokumen Kependudukan KTP \\nPertan...</td>\n",
       "      <td>Pengurusan offline dilakukan di kantor Dukcapi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.884073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Di mana saja perekaman e-KTP dapat dilakukan s...</td>\n",
       "      <td>[Kategori\\n: Dokumen Kependudukan KTP \\nPertan...</td>\n",
       "      <td>Perekaman e-KTP secara offline dapat dilakukan...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.281677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apa saja persyaratan umum dan dokumen yang har...</td>\n",
       "      <td>[Kategori\\n: Dokumen Kependudukan KTP \\nPertan...</td>\n",
       "      <td>Sebelum mengurus e-KTP, pastikan Anda telah be...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kapan seseorang wajib mengajukan penerbitan Ka...</td>\n",
       "      <td>[Kategori\\n:Kartu Keluarga (KK) \\nPertanyaan u...</td>\n",
       "      <td>Seseorang wajib mengajukan penerbitan KK baru ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.241582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagaimana cara mengurus Kartu Keluarga (KK) baru?</td>\n",
       "      <td>[Kategori\\n:Kartu Keluarga (KK) \\nPertanyaan u...</td>\n",
       "      <td>Untuk mengurus KK baru, pemohon perlu menyiapk...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apa saja syarat untuk membuat KK baru karena h...</td>\n",
       "      <td>[Kategori\\n:Kartu Keluarga (KK) \\nPertanyaan u...</td>\n",
       "      <td>Jika KK hilang, diperlukan surat keterangan hi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.280595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apa itu Kartu Identitas Anak (KIA)?</td>\n",
       "      <td>[Kategori\\n: Kartu Identitas Anak (KIA) \\nPert...</td>\n",
       "      <td>Kartu Identitas Anak (KIA) adalah kartu identi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.313684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apa syarat membuat KIA baru?</td>\n",
       "      <td>[Kategori\\n: Kartu Identitas Anak (KIA) \\nPert...</td>\n",
       "      <td>Syarat pembuatan KIA meliputi Kartu Keluarga (...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.226729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Di mana mengurus KIA?</td>\n",
       "      <td>[Kategori\\n: Kartu Identitas Anak (KIA) \\nPert...</td>\n",
       "      <td>Pengurusan KIA bisa dilakukan secara tatap muk...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Apa itu Akta Kelahiran?</td>\n",
       "      <td>[Kategori\\n: Akta Kelahiran \\nPertanyaan utama...</td>\n",
       "      <td>Akta Kelahiran adalah dokumen pencatatan sipil...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.865977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Di mana dan bagaimana cara mengurus Akta Kelah...</td>\n",
       "      <td>[Kategori\\n: Akta Kelahiran \\nPertanyaan utama...</td>\n",
       "      <td>Akta Kelahiran dapat diurus secara tatap muka ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Berapa biaya pengurusan Akta Kematian?</td>\n",
       "      <td>[Kategori\\n: Akta Kematian \\nPertanyaan utama ...</td>\n",
       "      <td>Pengurusan Akta Kematian dan layanan administr...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.298214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Apa syarat mengurus Akta Kematian?</td>\n",
       "      <td>[Kategori\\n: Akta Kematian \\nPertanyaan utama ...</td>\n",
       "      <td>Dokumen yang dibutuhkan antara lain surat kete...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.255725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Di mana mengurus Akta Kematian?</td>\n",
       "      <td>[Kategori\\n: Akta Kematian \\nPertanyaan utama ...</td>\n",
       "      <td>Permohonan dapat diajukan secara tatap muka di...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Berapa biaya pengurusan akta nikah?</td>\n",
       "      <td>[Kategori\\n: Akta nikah \\nPertanyaan utama / i...</td>\n",
       "      <td>Pengurusan akta nikah tidak dipungut biaya ata...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.290224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Apa syarat membuat akta nikah?</td>\n",
       "      <td>[Kategori\\n: Akta nikah \\nPertanyaan utama / i...</td>\n",
       "      <td>Syaratnya antara lain: surat nikah atau catata...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.244552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bagaimana cara mengurus akta nikah secara online?</td>\n",
       "      <td>[Kategori\\n: Akta nikah \\nPertanyaan utama / i...</td>\n",
       "      <td>Akta nikah bisa diurus secara online melalui w...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Apa perbedaan tempat mengurus perceraian bagi ...</td>\n",
       "      <td>[Kategori\\n: Akta cerai \\nPertanyaan utama / i...</td>\n",
       "      <td>Bagi pasangan suami istri yang beragama Islam,...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.244251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Apa syarat mengurus akta perceraian di Disdukc...</td>\n",
       "      <td>[Kategori\\n: Akta cerai \\nPertanyaan utama / i...</td>\n",
       "      <td>Dokumen yang diperlukan antara lain kutipan ak...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.245360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bagaimana cara mengurus akta cerai?</td>\n",
       "      <td>[Kategori\\n: Akta cerai \\nPertanyaan utama / i...</td>\n",
       "      <td>Akta cerai bisa diurus langsung di Kantor Dina...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Berapa nomor panggilan darurat Nasional di Ind...</td>\n",
       "      <td>[Kategori / topik\\n: Layanan publik kategori k...</td>\n",
       "      <td>Nomor panggilan darurat umum di Indonesia adal...</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Berapa jumlah rumah sakit yang dikelola oleh P...</td>\n",
       "      <td>[sakit kelas D, yaitu sebanyak 143. Berikutnya...</td>\n",
       "      <td>Berdasarkan data kepemilikan tahun 2024, ada 1...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Apa bedanya rumah sakit umum dan rumah sakit k...</td>\n",
       "      <td>[Kategori\\n: Kesehatan \\nPertanyaan utama / in...</td>\n",
       "      <td>Rumah Sakit Umum menyediakan pelayanan kesehat...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bagaimana pembagian kewenangan pengelolaan rum...</td>\n",
       "      <td>[sakit kelas D, yaitu sebanyak 143. Berikutnya...</td>\n",
       "      <td>Rumah sakit kelas A dikelola oleh Kementerian ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.216854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Apa itu Majadigi?</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Majadigi atau Majapahit Digital adalah platfor...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.215926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dimana bisa akses Majadigi?</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Aplikasi Majadigi tersedia dalam bentuk websit...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.217285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Seperti apa profil Jawa Timur</td>\n",
       "      <td>[Kategori\\n: Profil Jatim \\nPertanyaan utama /...</td>\n",
       "      <td>Ibu kota pemerintahan Provinsi Jawa Timur bera...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.235141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Apa visi misi Provinsi Jawa Timur saat ini?</td>\n",
       "      <td>[Kategori\\n: Visi Misi Provinsi Jawa Timur \\nP...</td>\n",
       "      <td>Visi Provinsi Jawa Timur adalah Jawa Timur Leb...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.223727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Apa makna visi Jawa Timur?</td>\n",
       "      <td>[Kategori\\n: Visi Misi Provinsi Jawa Timur \\nP...</td>\n",
       "      <td>Makna visi Jawa Timur tidak hanya menekankan k...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.277061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Seperti apa profil Khofifah Indar Parawansa?</td>\n",
       "      <td>[Kategori\\n: Profil Gubernur Jatim \\nPertanyaa...</td>\n",
       "      <td>Khofifah Indar Parawansa, perempuan kelahiran ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.281195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Siapa wakil Gubernur Jatim saat ini?</td>\n",
       "      <td>[Kategori\\n: Profil Wakil Gubernur Jatim \\nPer...</td>\n",
       "      <td>Wakil Gubernur Jawa Timur saat ini adalah Dr. ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.274666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Seperti apa profil Wagub Jatim, Emil Dardak?</td>\n",
       "      <td>[7. Perluasan pembiayaan usaha mikro dan kecil...</td>\n",
       "      <td>Emil Dardak, pria kelahiran 20 Mei 1984, diken...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.269324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Apa saja informasi yang diberikan BMKG?</td>\n",
       "      <td>[Kategori\\n: Prakiraan cuaca \\nPertanyaan utam...</td>\n",
       "      <td>BMKG menyediakan informasi cuaca harian (cerah...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Kapan jadwal operasional Bus Trans Jatim?</td>\n",
       "      <td>[Kategori\\n: Jadwal bus Trans Jatim \\nPertanya...</td>\n",
       "      <td>Bus Trans Jatim beroperasi setiap hari dengan ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.266347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Bagaimana cara membuat dan mencetak Kartu Nika...</td>\n",
       "      <td>[1. Foto suami dan istri \\n2. Nama pasangan \\n...</td>\n",
       "      <td>Kartu Nikah Digital bisa dibuat lewat situs si...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Apa itu Nawa Bhakti Satya dan programnya?</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Nawa Bhakti Satya berarti sembilan pengabdian ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.201455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Berapa tarif dan cara bayar bus Trans Jatim?</td>\n",
       "      <td>[Kategori\\n: Jadwal bus Trans Jatim \\nPertanya...</td>\n",
       "      <td>Tarif Trans Jatim sangat terjangkau:\\n\\nUmum R...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Di mana saja lokasi halte transit point Trans ...</td>\n",
       "      <td>[Bangkalan 1 - Ir Soekarno 1 - Pertigaan Marta...</td>\n",
       "      <td>Halte transit point Trans Jatim terbagi menjad...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.289689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Apa program prioritas Provinsi Jawa Timur 2025...</td>\n",
       "      <td>[Kategori / topik\\n:Program Prioritas (RPJMD) ...</td>\n",
       "      <td>Program prioritas Jatim 20252030 tertuang dal...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Apa saja program kerja prioritas Jatim Sejahtera?</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Program prioritas Jatim Sejahtera meliputi ASP...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Apa saja program kerja prioritas Jatim Akses d...</td>\n",
       "      <td>[7. Pembangunan dan Pengembangan Terminal Tipe...</td>\n",
       "      <td>Jatim Akses fokus pada peningkatan konektivita...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.290028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Apa saja program kerja prioritas Jatim Cerdas?</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Program prioritas Jatim Cerdas meliputi Jatim ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Apa saja program kerja prioritas Jatim Sehat?</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Program Jatim Sehat meliputi Jatim World Class...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.294077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Berikan Daftar Program Kerja Prioritas Jatim k...</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Jatim Kerja fokus membuka lapangan kerja berku...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Sebutkan program kerja prioritas Jatim Berkah ...</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Jatim Berkah - Amanah fokus pada tata kelola p...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Apa saja program kerja prioritas dari Jatim Agro?</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Jatim Agro berfokus pada peningkatan kesejahte...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.291018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Apa saja program kerja prioritas dari Jatim Ha...</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Jatim Harmoni berfokus pada pelestarian harmon...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.260104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Apa program utama dalam Jatim Lestari?</td>\n",
       "      <td>[Kategori / topik\\n: Program Kerja Prioritas J...</td>\n",
       "      <td>Jatim Lestari adalah bagian dari Nawa Bhakti S...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.263292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Apa saja layanan sosial yang tersedia di Majad...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Majadigi menyediakan layanan sosial seperti pe...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.272660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Apa saja layanan infrastruktur yang tersedia d...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Majadigi menyediakan layanan infrastruktur sep...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.261392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sebutkan layanan ekonomi &amp; bisnis yang tersedi...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Majadigi menyediakan layanan ekonomi dan bisni...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Apa saja layanan publik kategori Pemerintah da...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan Pemerintah...</td>\n",
       "      <td>Layanan publik Pemerintah dan Desa di Jawa Tim...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.275382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Layanan publik di bidang pariwisata &amp; kebudaya...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Layanan publik pariwisata &amp; kebudayaan di Maja...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.280282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Apa saja layanan publik bidang pendidikan di M...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Layanan publik pendidikan di Majadigi meliputi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.261076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Apa saja layanan publik bidang ketenagakerjaan?</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Layanan publik ketenagakerjaan di Majadigi men...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.289498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Apa saja layanan Kesehatan yang ada di Majadigi?</td>\n",
       "      <td>[[LINK] \\nMajadigi adalah platform digital res...</td>\n",
       "      <td>Layanan publik bidang Kesehatan di Majadigi me...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.273403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Layanan Kependudukan apa saja yang tersedia di...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Layanan publik bidang Kependudukan di Majadigi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.280727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Apa saja layanan publik bidang multisektoral K...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik kat...</td>\n",
       "      <td>Layanan publik bidang Multisektoral Khusus di ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.229698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Apa saja layanan Lingkungan Hidup yang ada di ...</td>\n",
       "      <td>[Kategori / topik\\n: Daftar layanan publik pad...</td>\n",
       "      <td>Layanan publik bidang Lingkungan Hidup di Maja...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Apa saja layanan publik berkaitan dengan keben...</td>\n",
       "      <td>[Kategori / topik\\n: Layanan publik kategori k...</td>\n",
       "      <td>Beberapa layanan publik di Majadigi yang masuk...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.256109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   Apa perbedaan pengurusan e-KTP online dan offl...   \n",
       "1   Di mana saja perekaman e-KTP dapat dilakukan s...   \n",
       "2   Apa saja persyaratan umum dan dokumen yang har...   \n",
       "3   Kapan seseorang wajib mengajukan penerbitan Ka...   \n",
       "4   Bagaimana cara mengurus Kartu Keluarga (KK) baru?   \n",
       "5   Apa saja syarat untuk membuat KK baru karena h...   \n",
       "6                 Apa itu Kartu Identitas Anak (KIA)?   \n",
       "7                        Apa syarat membuat KIA baru?   \n",
       "8                               Di mana mengurus KIA?   \n",
       "9                             Apa itu Akta Kelahiran?   \n",
       "10  Di mana dan bagaimana cara mengurus Akta Kelah...   \n",
       "11             Berapa biaya pengurusan Akta Kematian?   \n",
       "12                 Apa syarat mengurus Akta Kematian?   \n",
       "13                    Di mana mengurus Akta Kematian?   \n",
       "14                Berapa biaya pengurusan akta nikah?   \n",
       "15                     Apa syarat membuat akta nikah?   \n",
       "16  Bagaimana cara mengurus akta nikah secara online?   \n",
       "17  Apa perbedaan tempat mengurus perceraian bagi ...   \n",
       "18  Apa syarat mengurus akta perceraian di Disdukc...   \n",
       "19                Bagaimana cara mengurus akta cerai?   \n",
       "20  Berapa nomor panggilan darurat Nasional di Ind...   \n",
       "21  Berapa jumlah rumah sakit yang dikelola oleh P...   \n",
       "22  Apa bedanya rumah sakit umum dan rumah sakit k...   \n",
       "23  Bagaimana pembagian kewenangan pengelolaan rum...   \n",
       "24                                  Apa itu Majadigi?   \n",
       "25                        Dimana bisa akses Majadigi?   \n",
       "26                      Seperti apa profil Jawa Timur   \n",
       "27        Apa visi misi Provinsi Jawa Timur saat ini?   \n",
       "28                         Apa makna visi Jawa Timur?   \n",
       "29       Seperti apa profil Khofifah Indar Parawansa?   \n",
       "30               Siapa wakil Gubernur Jatim saat ini?   \n",
       "31       Seperti apa profil Wagub Jatim, Emil Dardak?   \n",
       "32            Apa saja informasi yang diberikan BMKG?   \n",
       "33          Kapan jadwal operasional Bus Trans Jatim?   \n",
       "34  Bagaimana cara membuat dan mencetak Kartu Nika...   \n",
       "35          Apa itu Nawa Bhakti Satya dan programnya?   \n",
       "36       Berapa tarif dan cara bayar bus Trans Jatim?   \n",
       "37  Di mana saja lokasi halte transit point Trans ...   \n",
       "38  Apa program prioritas Provinsi Jawa Timur 2025...   \n",
       "39  Apa saja program kerja prioritas Jatim Sejahtera?   \n",
       "40  Apa saja program kerja prioritas Jatim Akses d...   \n",
       "41     Apa saja program kerja prioritas Jatim Cerdas?   \n",
       "42      Apa saja program kerja prioritas Jatim Sehat?   \n",
       "43  Berikan Daftar Program Kerja Prioritas Jatim k...   \n",
       "44  Sebutkan program kerja prioritas Jatim Berkah ...   \n",
       "45  Apa saja program kerja prioritas dari Jatim Agro?   \n",
       "46  Apa saja program kerja prioritas dari Jatim Ha...   \n",
       "47             Apa program utama dalam Jatim Lestari?   \n",
       "48  Apa saja layanan sosial yang tersedia di Majad...   \n",
       "49  Apa saja layanan infrastruktur yang tersedia d...   \n",
       "50  Sebutkan layanan ekonomi & bisnis yang tersedi...   \n",
       "51  Apa saja layanan publik kategori Pemerintah da...   \n",
       "52  Layanan publik di bidang pariwisata & kebudaya...   \n",
       "53  Apa saja layanan publik bidang pendidikan di M...   \n",
       "54    Apa saja layanan publik bidang ketenagakerjaan?   \n",
       "55   Apa saja layanan Kesehatan yang ada di Majadigi?   \n",
       "56  Layanan Kependudukan apa saja yang tersedia di...   \n",
       "57  Apa saja layanan publik bidang multisektoral K...   \n",
       "58  Apa saja layanan Lingkungan Hidup yang ada di ...   \n",
       "59  Apa saja layanan publik berkaitan dengan keben...   \n",
       "\n",
       "                                   retrieved_contexts  \\\n",
       "0   [Kategori\\n: Dokumen Kependudukan KTP \\nPertan...   \n",
       "1   [Kategori\\n: Dokumen Kependudukan KTP \\nPertan...   \n",
       "2   [Kategori\\n: Dokumen Kependudukan KTP \\nPertan...   \n",
       "3   [Kategori\\n:Kartu Keluarga (KK) \\nPertanyaan u...   \n",
       "4   [Kategori\\n:Kartu Keluarga (KK) \\nPertanyaan u...   \n",
       "5   [Kategori\\n:Kartu Keluarga (KK) \\nPertanyaan u...   \n",
       "6   [Kategori\\n: Kartu Identitas Anak (KIA) \\nPert...   \n",
       "7   [Kategori\\n: Kartu Identitas Anak (KIA) \\nPert...   \n",
       "8   [Kategori\\n: Kartu Identitas Anak (KIA) \\nPert...   \n",
       "9   [Kategori\\n: Akta Kelahiran \\nPertanyaan utama...   \n",
       "10  [Kategori\\n: Akta Kelahiran \\nPertanyaan utama...   \n",
       "11  [Kategori\\n: Akta Kematian \\nPertanyaan utama ...   \n",
       "12  [Kategori\\n: Akta Kematian \\nPertanyaan utama ...   \n",
       "13  [Kategori\\n: Akta Kematian \\nPertanyaan utama ...   \n",
       "14  [Kategori\\n: Akta nikah \\nPertanyaan utama / i...   \n",
       "15  [Kategori\\n: Akta nikah \\nPertanyaan utama / i...   \n",
       "16  [Kategori\\n: Akta nikah \\nPertanyaan utama / i...   \n",
       "17  [Kategori\\n: Akta cerai \\nPertanyaan utama / i...   \n",
       "18  [Kategori\\n: Akta cerai \\nPertanyaan utama / i...   \n",
       "19  [Kategori\\n: Akta cerai \\nPertanyaan utama / i...   \n",
       "20  [Kategori / topik\\n: Layanan publik kategori k...   \n",
       "21  [sakit kelas D, yaitu sebanyak 143. Berikutnya...   \n",
       "22  [Kategori\\n: Kesehatan \\nPertanyaan utama / in...   \n",
       "23  [sakit kelas D, yaitu sebanyak 143. Berikutnya...   \n",
       "24  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "25  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "26  [Kategori\\n: Profil Jatim \\nPertanyaan utama /...   \n",
       "27  [Kategori\\n: Visi Misi Provinsi Jawa Timur \\nP...   \n",
       "28  [Kategori\\n: Visi Misi Provinsi Jawa Timur \\nP...   \n",
       "29  [Kategori\\n: Profil Gubernur Jatim \\nPertanyaa...   \n",
       "30  [Kategori\\n: Profil Wakil Gubernur Jatim \\nPer...   \n",
       "31  [7. Perluasan pembiayaan usaha mikro dan kecil...   \n",
       "32  [Kategori\\n: Prakiraan cuaca \\nPertanyaan utam...   \n",
       "33  [Kategori\\n: Jadwal bus Trans Jatim \\nPertanya...   \n",
       "34  [1. Foto suami dan istri \\n2. Nama pasangan \\n...   \n",
       "35  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "36  [Kategori\\n: Jadwal bus Trans Jatim \\nPertanya...   \n",
       "37  [Bangkalan 1 - Ir Soekarno 1 - Pertigaan Marta...   \n",
       "38  [Kategori / topik\\n:Program Prioritas (RPJMD) ...   \n",
       "39  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "40  [7. Pembangunan dan Pengembangan Terminal Tipe...   \n",
       "41  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "42  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "43  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "44  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "45  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "46  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "47  [Kategori / topik\\n: Program Kerja Prioritas J...   \n",
       "48  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "49  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "50  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "51  [Kategori / topik\\n: Daftar layanan Pemerintah...   \n",
       "52  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "53  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "54  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "55  [[LINK] \\nMajadigi adalah platform digital res...   \n",
       "56  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "57  [Kategori / topik\\n: Daftar layanan publik kat...   \n",
       "58  [Kategori / topik\\n: Daftar layanan publik pad...   \n",
       "59  [Kategori / topik\\n: Layanan publik kategori k...   \n",
       "\n",
       "                                            reference  context_recall  \\\n",
       "0   Pengurusan offline dilakukan di kantor Dukcapi...        1.000000   \n",
       "1   Perekaman e-KTP secara offline dapat dilakukan...        1.000000   \n",
       "2   Sebelum mengurus e-KTP, pastikan Anda telah be...        1.000000   \n",
       "3   Seseorang wajib mengajukan penerbitan KK baru ...        0.666667   \n",
       "4   Untuk mengurus KK baru, pemohon perlu menyiapk...        1.000000   \n",
       "5   Jika KK hilang, diperlukan surat keterangan hi...        1.000000   \n",
       "6   Kartu Identitas Anak (KIA) adalah kartu identi...        1.000000   \n",
       "7   Syarat pembuatan KIA meliputi Kartu Keluarga (...        1.000000   \n",
       "8   Pengurusan KIA bisa dilakukan secara tatap muk...        1.000000   \n",
       "9   Akta Kelahiran adalah dokumen pencatatan sipil...        1.000000   \n",
       "10  Akta Kelahiran dapat diurus secara tatap muka ...        1.000000   \n",
       "11  Pengurusan Akta Kematian dan layanan administr...        1.000000   \n",
       "12  Dokumen yang dibutuhkan antara lain surat kete...        1.000000   \n",
       "13  Permohonan dapat diajukan secara tatap muka di...        1.000000   \n",
       "14  Pengurusan akta nikah tidak dipungut biaya ata...        1.000000   \n",
       "15  Syaratnya antara lain: surat nikah atau catata...        1.000000   \n",
       "16  Akta nikah bisa diurus secara online melalui w...        1.000000   \n",
       "17  Bagi pasangan suami istri yang beragama Islam,...        1.000000   \n",
       "18  Dokumen yang diperlukan antara lain kutipan ak...        1.000000   \n",
       "19  Akta cerai bisa diurus langsung di Kantor Dina...        1.000000   \n",
       "20  Nomor panggilan darurat umum di Indonesia adal...        0.961538   \n",
       "21  Berdasarkan data kepemilikan tahun 2024, ada 1...        1.000000   \n",
       "22  Rumah Sakit Umum menyediakan pelayanan kesehat...        1.000000   \n",
       "23  Rumah sakit kelas A dikelola oleh Kementerian ...        1.000000   \n",
       "24  Majadigi atau Majapahit Digital adalah platfor...        1.000000   \n",
       "25  Aplikasi Majadigi tersedia dalam bentuk websit...        1.000000   \n",
       "26  Ibu kota pemerintahan Provinsi Jawa Timur bera...        1.000000   \n",
       "27  Visi Provinsi Jawa Timur adalah Jawa Timur Leb...        1.000000   \n",
       "28  Makna visi Jawa Timur tidak hanya menekankan k...        1.000000   \n",
       "29  Khofifah Indar Parawansa, perempuan kelahiran ...        1.000000   \n",
       "30  Wakil Gubernur Jawa Timur saat ini adalah Dr. ...        1.000000   \n",
       "31  Emil Dardak, pria kelahiran 20 Mei 1984, diken...        1.000000   \n",
       "32  BMKG menyediakan informasi cuaca harian (cerah...        1.000000   \n",
       "33  Bus Trans Jatim beroperasi setiap hari dengan ...        1.000000   \n",
       "34  Kartu Nikah Digital bisa dibuat lewat situs si...        1.000000   \n",
       "35  Nawa Bhakti Satya berarti sembilan pengabdian ...        1.000000   \n",
       "36  Tarif Trans Jatim sangat terjangkau:\\n\\nUmum R...        1.000000   \n",
       "37  Halte transit point Trans Jatim terbagi menjad...        0.666667   \n",
       "38  Program prioritas Jatim 20252030 tertuang dal...        1.000000   \n",
       "39  Program prioritas Jatim Sejahtera meliputi ASP...        1.000000   \n",
       "40  Jatim Akses fokus pada peningkatan konektivita...        1.000000   \n",
       "41  Program prioritas Jatim Cerdas meliputi Jatim ...        0.000000   \n",
       "42  Program Jatim Sehat meliputi Jatim World Class...        1.000000   \n",
       "43  Jatim Kerja fokus membuka lapangan kerja berku...        1.000000   \n",
       "44  Jatim Berkah - Amanah fokus pada tata kelola p...        1.000000   \n",
       "45  Jatim Agro berfokus pada peningkatan kesejahte...        1.000000   \n",
       "46  Jatim Harmoni berfokus pada pelestarian harmon...        1.000000   \n",
       "47  Jatim Lestari adalah bagian dari Nawa Bhakti S...        1.000000   \n",
       "48  Majadigi menyediakan layanan sosial seperti pe...        1.000000   \n",
       "49  Majadigi menyediakan layanan infrastruktur sep...        1.000000   \n",
       "50  Majadigi menyediakan layanan ekonomi dan bisni...        1.000000   \n",
       "51  Layanan publik Pemerintah dan Desa di Jawa Tim...        1.000000   \n",
       "52  Layanan publik pariwisata & kebudayaan di Maja...        1.000000   \n",
       "53  Layanan publik pendidikan di Majadigi meliputi...        0.000000   \n",
       "54  Layanan publik ketenagakerjaan di Majadigi men...        1.000000   \n",
       "55  Layanan publik bidang Kesehatan di Majadigi me...        0.250000   \n",
       "56  Layanan publik bidang Kependudukan di Majadigi...        1.000000   \n",
       "57  Layanan publik bidang Multisektoral Khusus di ...        1.000000   \n",
       "58  Layanan publik bidang Lingkungan Hidup di Maja...        0.500000   \n",
       "59  Beberapa layanan publik di Majadigi yang masuk...        0.000000   \n",
       "\n",
       "    context_precision   latency  \n",
       "0                 1.0  0.884073  \n",
       "1                 1.0  0.281677  \n",
       "2                 1.0  0.264497  \n",
       "3                 1.0  0.241582  \n",
       "4                 1.0  0.227664  \n",
       "5                 1.0  0.280595  \n",
       "6                 1.0  0.313684  \n",
       "7                 1.0  0.226729  \n",
       "8                 1.0  0.250192  \n",
       "9                 1.0  1.865977  \n",
       "10                1.0  0.205112  \n",
       "11                1.0  0.298214  \n",
       "12                1.0  0.255725  \n",
       "13                1.0  0.228043  \n",
       "14                1.0  0.290224  \n",
       "15                1.0  0.244552  \n",
       "16                1.0  0.221307  \n",
       "17                1.0  0.244251  \n",
       "18                1.0  0.245360  \n",
       "19                1.0  0.285323  \n",
       "20                1.0  0.227348  \n",
       "21                1.0  0.220286  \n",
       "22                1.0  0.222414  \n",
       "23                0.5  0.216854  \n",
       "24                1.0  0.215926  \n",
       "25                1.0  0.217285  \n",
       "26                1.0  0.235141  \n",
       "27                1.0  0.223727  \n",
       "28                1.0  0.277061  \n",
       "29                1.0  0.281195  \n",
       "30                1.0  0.274666  \n",
       "31                0.5  0.269324  \n",
       "32                1.0  0.265649  \n",
       "33                1.0  0.266347  \n",
       "34                1.0  0.391927  \n",
       "35                1.0  1.201455  \n",
       "36                1.0  0.269957  \n",
       "37                1.0  0.289689  \n",
       "38                1.0  0.250317  \n",
       "39                1.0  0.269730  \n",
       "40                1.0  0.290028  \n",
       "41                0.0  0.363563  \n",
       "42                1.0  0.294077  \n",
       "43                1.0  0.271313  \n",
       "44                1.0  0.258751  \n",
       "45                1.0  0.291018  \n",
       "46                1.0  0.260104  \n",
       "47                1.0  0.263292  \n",
       "48                1.0  0.272660  \n",
       "49                1.0  0.261392  \n",
       "50                1.0  0.305080  \n",
       "51                1.0  0.275382  \n",
       "52                1.0  0.280282  \n",
       "53                1.0  0.261076  \n",
       "54                1.0  0.289498  \n",
       "55                1.0  0.273403  \n",
       "56                1.0  0.280727  \n",
       "57                1.0  0.229698  \n",
       "58                1.0  0.283108  \n",
       "59                1.0  0.256109  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions_list = []\n",
    "contexts_list = []\n",
    "references_list = []\n",
    "latency_list = []\n",
    "df_to_eval = df_eval.iloc[0:60]\n",
    "\n",
    "for row in tqdm(df_to_eval.itertuples(), total=len(df_to_eval), desc=f\"Loading {len(df_to_eval)} Data\"):\n",
    "    question = row.Pertanyaan\n",
    "    ground_truth = row.Jawaban\n",
    "    context, latency = get_answer_context(question)\n",
    "    if context is not None:\n",
    "        questions_list.append(question)\n",
    "        contexts_list.append(context)\n",
    "        references_list.append(ground_truth)\n",
    "        latency_list.append(latency)\n",
    "    else:\n",
    "        print(f\"Melewatkan pertanyaan karena error API: '{question}'\")\n",
    "\n",
    "if not questions_list:\n",
    "    print(\"\\nTidak ada data yang berhasil diproses dari API.\")\n",
    "else:\n",
    "    data = {\n",
    "        \"question\": questions_list,\n",
    "        \"contexts\": contexts_list,\n",
    "        \"reference\": references_list \n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    print(f\"Evaluate RAGAs pada {len(dataset)} data\")\n",
    "    results = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=[LLMContextRecall(), ContextPrecision()],\n",
    "        llm=llm_ragas\n",
    "    )\n",
    "    df_result = results.to_pandas()\n",
    "    latency_list_float = []\n",
    "    for x in latency_list:\n",
    "        try:\n",
    "            latency_list_float.append(float(x))\n",
    "        except:\n",
    "            latency_list_float.append(None)\n",
    "    df_result[\"latency\"] = latency_list_float\n",
    "    print(\"--- Hasil Evaluasi RAGAs ---\")\n",
    "    display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rata-rata Recall: 0.9174145299145299\n",
      "rata-rata Precision: 0.9666666666016667\n",
      "rata-rata Latency: 0.3166940768559774\n"
     ]
    }
   ],
   "source": [
    "recall_mean = df_result['context_recall'].mean()\n",
    "print(f\"rata-rata Recall: {recall_mean}\")\n",
    "precision_mean = df_result['context_precision'].mean()\n",
    "print(f\"rata-rata Precision: {precision_mean}\")\n",
    "latency_mean = df_result['latency'].mean()\n",
    "print(f\"rata-rata Latency: {latency_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(\"Parent-Child-Vector-Search.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7513fd8eaa620e06c80ba873963700a9648020412bf3a060c767761d357c4158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
