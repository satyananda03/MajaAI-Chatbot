{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "EMBEDDING_PROVIDER=\"aws\"\n",
    "EMBEDDING_MODEL_ID=\"amazon.titan-embed-text-v2:0\"\n",
    "LLM_PROVIDER=\"aws\"\n",
    "LLM_ID = \"apac.amazon.nova-lite-v1:0\"\n",
    "MAX_TOKEN = 1500\n",
    "TEMPERATURE = 0.0\n",
    "STREAMING = True\n",
    "\n",
    "# DATABASE\n",
    "REDIS_URL=\"redis://localhost:6379\"\n",
    "PERSIST_DIR=\"./data/chroma_dbs\"\n",
    "COLLECTION_NAME=\"parent-child-chunk-v1\"\n",
    "BM25_INDEX_PATH = \"./data/bm25-index/bm25-index-v1.pkl\"\n",
    "STOPWORDS_PATH = \"./data/stopwords.txt\"\n",
    "\n",
    "# RETRIEVER\n",
    "RETRIEVAL_STRATEGY: str = \"hybrid\" \n",
    "MAX_RESULTS: int = 2\n",
    "BM25_SEARCH_K: int = 5\n",
    "VECTOR_SEARCH_K: int = 80\n",
    "BM25_WEIGHT: float = 0.3\n",
    "VECTOR_WEIGHT: float = 0.7 \n",
    "RRF_CONSTANT: int = 5\n",
    "SCORE_THRESHOLD: float = 0.01\n",
    "\n",
    "# PROMPT\n",
    "PROMPT_VERSION = \"v1\"\n",
    "PROMPT_DIR = \"./prompts\"\n",
    "\n",
    "# SPLITTER\n",
    "PARENT_CHUNK_SIZE=2400\n",
    "PARENT_CHUNK_OVERLAP=260\n",
    "CHILD_CHUNK_SIZE=300\n",
    "CHILD_CHUNK_OVERLAP=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class PDFFileLoader:\n",
    "    def load(self, source: str) -> List[Document]:\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(file_path=source)\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata[\"source_type\"] = \"pdf\"\n",
    "                doc.metadata[\"file_path\"] = source\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Gagal memuat PDF {source}: {str(e)}\"\n",
    "            raise RuntimeError(error_msg) from e\n",
    "\n",
    "class PDFDirectoryLoader:\n",
    "    def __init__(self):\n",
    "        self.single_loader = PDFFileLoader()\n",
    "    def load(self, source: str, recursive: bool = True) -> List[Document]:\n",
    "        documents: List[Document] = []\n",
    "        search_pattern = os.path.join(source, \"*.pdf\")\n",
    "        # Ambil list semua file path\n",
    "        pdf_files = glob.glob(search_pattern)\n",
    "        if not pdf_files:\n",
    "            return []\n",
    "        # Loop setiap file di dalam direktori\n",
    "        for file_path in pdf_files:\n",
    "            # Skip hidden files\n",
    "            if \"/.\" in file_path or \"\\\\.\" in file_path:\n",
    "                continue\n",
    "            # Panggil loader satuan\n",
    "            docs = self.single_loader.load(file_path)\n",
    "            documents.extend(docs)\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class ParentChildSplitter:\n",
    "    def __init__(self, parent_chunk_size: int, parent_chunk_overlap: int, child_chunk_size: int, child_chunk_overlap: int):\n",
    "        self.parent_splitter = RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size,\n",
    "                                                            chunk_overlap=parent_chunk_overlap,\n",
    "                                                            separators=[\"\\n\", \". \", \", \", \" \", \"\"])\n",
    "        self.child_splitter = RecursiveCharacterTextSplitter(chunk_size=child_chunk_size,\n",
    "                                                            chunk_overlap=child_chunk_overlap,\n",
    "                                                            separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"])\n",
    "                                                            \n",
    "    def _preprocess_text(self, chunk_text: str) -> Tuple[str, List[str]]:\n",
    "        # Ekstrak URL\n",
    "        urls = re.findall(r'https?://\\S+', chunk_text)\n",
    "        # Replace URL dengan placeholder LINK\n",
    "        text_clean = re.sub(r'https?://\\S+', '[LINK]', chunk_text)\n",
    "        # Hapus karakter yang tidak diinginkan tapi\n",
    "        text_clean = re.sub(r'[^\\w\\s\\[\\],.:()%=+\\-/]', '', text_clean)\n",
    "        # Rapikan newline ganda > 2 menjadi 1\n",
    "        text_clean = re.sub(r'\\n{2,}', '\\n', text_clean)\n",
    "        # Rapikan spasi berlebihan\n",
    "        text_clean = re.sub(r'[ ]{2,}', ' ', text_clean)\n",
    "        # Fix multiple blank lines -> single newline\n",
    "        text_clean = re.sub(r'\\n\\s*\\n+', '\\n', text_clean)\n",
    "        return text_clean.strip(), urls\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> Dict[str, List[Document]]:\n",
    "        # Grouping by Source/File Path\n",
    "        docs_by_source = defaultdict(list)\n",
    "        for doc in documents:\n",
    "            source_key = doc.metadata.get(\"file_path\")\n",
    "            docs_by_source[source_key].append(doc)\n",
    "        all_parent_docs = []\n",
    "        all_child_docs = []\n",
    "        # Proses per File Source\n",
    "        for source_name, doc_group in docs_by_source.items():\n",
    "            # Gabungkan semua text dari satu file (merge pages)\n",
    "            combined_text = \"\\n\".join([d.page_content for d in doc_group])\n",
    "            # Ambil metadata dasar dari halaman pertama\n",
    "            base_metadata = doc_group[0].metadata.copy() if doc_group else {}\n",
    "            # Buat satu dokumen besar sementara\n",
    "            combined_doc = Document(page_content=combined_text, metadata=base_metadata)\n",
    "            # Generate Parent Chunks\n",
    "            parent_chunks = self.parent_splitter.split_documents([combined_doc])\n",
    "            for p_doc in parent_chunks:\n",
    "                # Cleaning Text\n",
    "                clean_text, specific_urls = self._preprocess_text(p_doc.page_content)\n",
    "                # Generate Parent ID (UUID)\n",
    "                parent_id = str(uuid.uuid4())\n",
    "                # Update Metadata Parent\n",
    "                parent_meta = p_doc.metadata.copy()\n",
    "                parent_meta.update({\n",
    "                    \"doc_id\": parent_id,\n",
    "                    \"type\": \"parent\",\n",
    "                    \"source\": source_name,\n",
    "                    \"urls\": specific_urls, \n",
    "                })\n",
    "                final_parent_doc = Document(page_content=clean_text, metadata=parent_meta)\n",
    "                all_parent_docs.append(final_parent_doc)\n",
    "                # Generate Child Chunk dari Parent Chunk\n",
    "                child_texts = self.child_splitter.split_text(clean_text)\n",
    "                for c_text in child_texts:\n",
    "                    child_meta = {\n",
    "                        \"parent_id\": parent_id,\n",
    "                        \"type\": \"child\",\n",
    "                        \"source\": source_name,\n",
    "                    }\n",
    "                    child_doc = Document(page_content=c_text, metadata=child_meta)\n",
    "                    all_child_docs.append(child_doc)\n",
    "\n",
    "        return {\"parents\": all_parent_docs,\n",
    "                \"children\": all_child_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_embeddings(provider: str = EMBEDDING_PROVIDER, model_id: str = EMBEDDING_MODEL_ID) -> Embeddings:\n",
    "    if provider == \"aws\":\n",
    "        return BedrockEmbeddings(model_id=model_id,     \n",
    "                                region_name=os.getenv(\"AWS_REGION\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Provider embedding tidak didukung\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "def encode(doc: Document) -> bytes:\n",
    "    return dumps(doc).encode(\"utf-8\")\n",
    "\n",
    "def decode(data: bytes) -> Document:\n",
    "    if isinstance(data, str):\n",
    "        data = data.encode(\"utf-8\")\n",
    "    return loads(data.decode(\"utf-8\"))\n",
    "\n",
    "def encode_key(key: Any) -> str:\n",
    "    if isinstance(key, bytes):\n",
    "        return key.decode(\"utf-8\")\n",
    "    return str(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class VectorStore:\n",
    "    @staticmethod\n",
    "    def get_vector_store(embedding_model: Embeddings, collection_name: str = COLLECTION_NAME, persist_directory: str = PERSIST_DIR):\n",
    "        return Chroma(embedding_function=embedding_model, \n",
    "                    persist_directory=persist_directory,\n",
    "                    collection_name=collection_name,\n",
    "                    collection_metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain.storage import EncoderBackedStore\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DocStore:\n",
    "    @staticmethod\n",
    "    def get_doc_store(collection_name: str = COLLECTION_NAME, redis_url: str = REDIS_URL):\n",
    "        namespace = f\"docstore_{collection_name}\"\n",
    "        try:\n",
    "            logger.info(f\"Connecting to Redis DocStore {redis_url}, Namespace: {namespace})\")\n",
    "            raw_store = RedisStore(redis_url=redis_url, namespace=namespace)\n",
    "            return EncoderBackedStore(store=raw_store,\n",
    "                                    key_encoder=encode_key,\n",
    "                                    value_serializer=encode,\n",
    "                                    value_deserializer=decode)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal koneksi ke Redis {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from typing import Set, List\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_stopwords(path: str) -> Set[str]:\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                stopwords = {line.strip().lower() for line in f if line.strip()}\n",
    "            logger.info(\"Stopwords berhasil di load\")\n",
    "            return stopwords\n",
    "        else:\n",
    "            logger.warning(f\"Stopword file tidak ditemukan {path}\")\n",
    "            return set()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gagal load stopwords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def preprocess_text(text: str, stopwords: Set[str]) -> List[str]:\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"[a-z0-9]+(?:-[a-z0-9]+)*\", text)\n",
    "    if stopwords:\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent-Child Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import pickle\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from typing import List, Dict\n",
    "from functools import partial\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ParentChildIndexer:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = get_embeddings()\n",
    "        self.collection_name = COLLECTION_NAME\n",
    "        self.vector_store = VectorStore.get_vector_store(\n",
    "            embedding_model=self.embedding_model,\n",
    "            collection_name=self.collection_name\n",
    "        )\n",
    "        self.doc_store = DocStore.get_doc_store(collection_name=self.collection_name)\n",
    "\n",
    "    def index_documents(self, split_result: Dict[str, List[Document]]):\n",
    "        parent_docs = split_result.get(\"parents\", [])\n",
    "        child_docs = split_result.get(\"children\", [])\n",
    "        logger.info(f\"Indexing {len(parent_docs)} Parents & {len(child_docs)} Child\")\n",
    "        # SIMPAN PARENTS KE REDIS\n",
    "        if parent_docs:\n",
    "            try:\n",
    "                parent_key_value_pairs = []\n",
    "                for doc in parent_docs:\n",
    "                    doc_id = doc.metadata.get(\"doc_id\") or str(uuid.uuid4())\n",
    "                    doc.metadata[\"doc_id\"] = doc_id\n",
    "                    parent_key_value_pairs.append((doc_id, doc))\n",
    "                self.doc_store.mset(parent_key_value_pairs)\n",
    "                logger.info(f\"Berhasil menyimpan {len(parent_docs)} Parent Chunk ke Redis.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error simpan Parent Chunk ke Redis: {e}\")\n",
    "                raise e\n",
    "        # SIMPAN CHILDREN KE CHROMA\n",
    "        if child_docs:\n",
    "            try:\n",
    "                valid_children = [d for d in child_docs if \"parent_id\" in d.metadata]\n",
    "                self.vector_store.add_documents(valid_children)\n",
    "                logger.info(f\"Berhasil menyimpan {len(valid_children)} Child Chunk ke ChromaDB.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error simpan Children Chunk ke Chroma: {e}\")\n",
    "                raise e\n",
    "        return {\"Parents indexed\": len(parent_docs), \"Children indexed\": len(child_docs)}\n",
    "\n",
    "class BM25Indexer:\n",
    "    def __init__(self):\n",
    "        self.vector_store = VectorStore.get_vector_store(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            embedding_model=get_embeddings()\n",
    "        )\n",
    "    def build_and_save_index(self):\n",
    "        # 1. Load stopwords\n",
    "        stopwords = load_stopwords(STOPWORDS_PATH)\n",
    "        # 2. Fetch documents child dari vectordb\n",
    "        try:\n",
    "            result = self.vector_store.get(\n",
    "                where={\"type\": \"child\"},\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal fetch data dari Chroma: {e}\")\n",
    "            raise e\n",
    "        raw_docs = result.get(\"documents\", [])\n",
    "        raw_metadatas = result.get(\"metadatas\", [])\n",
    "        if not raw_docs:\n",
    "            logger.warning(\"Tidak ditemukan dokumen 'child' di Chroma\")\n",
    "            return\n",
    "        logger.info(f\"Indexing {len(raw_docs)} dokumen\")\n",
    "        # 3. Convert ke LangChain Document\n",
    "        documents = [\n",
    "            Document(page_content=text, metadata=meta)\n",
    "            for text, meta in zip(raw_docs, raw_metadatas)\n",
    "        ]\n",
    "        # 4. Build BM25 retriever\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents, preprocess_func=partial(preprocess_text, stopwords=stopwords))\n",
    "        # 5. Save index\n",
    "        try:\n",
    "            with open(BM25_INDEX_PATH, \"wb\") as f:\n",
    "                pickle.dump(bm25_retriever, f)\n",
    "            logger.info(f\"BM25 Index berhasil disimpan di: {BM25_INDEX_PATH}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal menyimpan file pickle: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run_ingestion_pipeline(folder_path: str):\n",
    "    print(\"Loading Data\")\n",
    "    loader = PDFDirectoryLoader()\n",
    "    raw_docs = loader.load(folder_path)\n",
    "    print(\"Splitting Dokumen\")\n",
    "    splitter = ParentChildSplitter(\n",
    "        parent_chunk_size=PARENT_CHUNK_SIZE,\n",
    "        parent_chunk_overlap=PARENT_CHUNK_OVERLAP,\n",
    "        child_chunk_size=CHILD_CHUNK_SIZE,\n",
    "        child_chunk_overlap=CHILD_CHUNK_OVERLAP\n",
    "    )\n",
    "    split_result = splitter.split_documents(raw_docs)\n",
    "    print(\"Indexing Document Chunks\")\n",
    "    vector_indexer = ParentChildIndexer()\n",
    "    result = vector_indexer.index_documents(split_result)\n",
    "    print(f\"{result}\")\n",
    "    print(\"Building BM25 Index\")\n",
    "    keyword_indexer = BM25Indexer()\n",
    "    keyword_indexer.build_and_save_index()\n",
    "    print(\"Ingestion DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Splitting Dokumen\n",
      "Indexing Document Chunks\n",
      "{'Parents indexed': 90, 'Children indexed': 631}\n",
      "Building BM25 Index\n",
      "Ingestion DONE\n"
     ]
    }
   ],
   "source": [
    "run_ingestion_pipeline(\"MajaAI_Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7513fd8eaa620e06c80ba873963700a9648020412bf3a060c767761d357c4158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
